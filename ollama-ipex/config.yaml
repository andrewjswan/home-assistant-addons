name: Ollama IPEX
description: Get up and running with large language models.
version: 0.5.7
slug: ollama-ipex
codenotary: notary@home-assistant.io
arch:
- aarch64
- amd64
url: https://ollama.com
map:
- config:rw
- share:rw
image: ghcr.io/andrewjswan/ollama-ipex-addon-{arch}
init: false
ingress: true
legacy: true
video: true
ingress_port: 11434
ingress_stream: true
ports:
  11434/tcp: 11434
ports_description:
  11434/tcp: The Ollama API Port, not used for End-User Access
options:
  OLLAMA_MODELS: /share/ollama
  OLLAMA_KEEP_ALIVE: -1
  OLLAMA_KV_CACHE_TYPE: q8_0
  OLLAMA_FLASH_ATTENTION: 0
  OLLAMA_MAX_LOADED_MODELS: 1
  OLLAMA_NUM_PARALLEL: 1
  OLLAMA_ORIGINS: ''
  OLLAMA_DEBUG: 1
  gpus: all
schema:
  OLLAMA_FLASH_ATTENTION: int
  OLLAMA_MODELS: list(/config/ollama|/share/ollama)
  OLLAMA_KEEP_ALIVE: int
  OLLAMA_KV_CACHE_TYPE: list(f16|q8_0|q4_0)
  OLLAMA_MAX_LOADED_MODELS: int
  OLLAMA_NUM_PARALLEL: int
  OLLAMA_ORIGINS: str
  OLLAMA_DEBUG: int
  gpus: str
environment:
  OLLAMA_NUM_GPU: '999'
  ZES_ENABLE_SYSMAN: '1'
  SYCL_CACHE_PERSISTENT: '1'
  SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS: '1'
  ONEAPI_DEVICE_SELECTOR: level_zero:0
  IPEX_LLM_MODEL_SOURCE: modelscope
  OLLAMA_SET_OT: exps=CPU
devices:
- /dev/dri/renderD128
- /dev/apex_0
- /dev/apex_1
- /dev/apex_2
- /dev/apex_3
- /dev/dri/card0
- /dev/vchiq
- /dev/video10
- /dev/video0
startup: application
watchdog: tcp://[HOST]:[PORT:11434]
